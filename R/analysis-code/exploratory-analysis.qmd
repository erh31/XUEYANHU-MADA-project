---
title: "An example exploratory analysis script"
author: "Xueyan Hu"
date: "03/22/2024"
output: html_document
---

  
  
This Quarto file loads the cleaned data and does some exploring.

I'm only showing it the way where the code is included in the file. 
As described in the `processing_code` materials, I currently prefer the approach of having R code in a separate file and pulling it in.

But I already had this written and haven't yet re-done it that way. Feel free to redo and send a pull request on GitHub :)

Again, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files.
And sometimes, an R script with enough comments is good enough and one doesn't need a Quarto file.

Also note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you'll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.

As part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.

Start by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables. 

Plots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.

# Setup

```{r}
#load needed packages. make sure they are installed.
library(here) #for data loading/saving
library(dplyr)
library(skimr)
library(ggplot2)
library(tidymodels)
library(discrim)
```


Load the data.

```{r}
#Path to data. Note the use of the here() package and not absolute paths
data_location <- here::here("data","processed-data","processeddata.rds")
#load data
mydata <- readRDS(data_location)
```



# fit simple linear model

First, I am curious that if physical activity could be correlated to the choice of transportation since I have heard a theory that if someone take much effort on commute transportation, they tend to spend less time on exercise.
First I want to use number to represent the type of transportation. Walking requires most energy consumption, then it goes with public transportation and automobile is the least.

```{r}
# Define a function to map transportation modes to numeric values
map_transport <- function(transport_mode) {
  if (transport_mode == "Walking") {
    return(4)
  } else if (transport_mode == "Motorbike") {
    return(2)
  } else if (transport_mode == "Public_Transportation") {
    return(3)
  } else if (transport_mode == "Automobile") {
    return(1)
  } else {
    return(0)  # Return 0 for other cases
  }
}

# Apply the function to create the new variable
mydata$MTRANSn <- sapply(mydata$MTRANS, map_transport)

# Calculate correlation coefficient
correlation <- cor(mydata$FAF, mydata$MTRANSn)

# Print correlation coefficient
print(correlation)
```
Since the correlation coefficient is close to zero, there is almost no linear association between them. So this result doesn't really support the theory that I have heard from social media.


Then, I would like to see how drinking water plus physical activity affect obesity.

```{r}
# linear model of BMI by water and FAF
lm1 <- lm(BMI ~ Water + FAF + Water * FAF, data = mydata)
lm1_result <- summary(lm1)
lm1_result

# save the result
lm_file = here("results", "tables", "lm1table.rds")
saveRDS(lm1_result, file = lm_file)
```

In addition, I want to fit model for alcohol consumption and obesity level.

```{r}
mydata$Alcohol <- as.factor(mydata$Alcohol)
lm2 <- lm(BMI ~ Alcohol, data = mydata)
lm2_result <- summary(lm2)

# save the result
lm_file = here("results", "tables", "lm2table.rds")
saveRDS(lm2_result, file = lm_file)
```
I think it makes a little bit sense to me that there is a connection effect of water and physical activity on BMI, which might be supportive for what we learn that drinking more water and doing more exercise will lower the risk of obesity.

I haven't include gender or age for model fitting, I will try to combine them in one model here.

```{r}
lm4 <- lm(BMI ~ Gender * Age, data = mydata)
lm4_result <- summary(lm4)
lm4_result

# save the result
lm_file = here("results", "tables", "lm3table.rds")
saveRDS(lm3_result, file = lm_file)
```

This linear model proves gender and age can also be predictors for BMI and there is interaction between them. So I will include them in the all predictor model.


Last, I will try to use all predictors that I believe may affect BMI to fit a model.

```{r}
lm3 <- lm(BMI ~ Gender + Age + History + Water + Alcohol + FAF + MTRANS, data = mydata)
lm3_result <- summary(lm3)
lm3_result

# save the result
lm_file = here("results", "tables", "lm3table.rds")
saveRDS(lm3_result, file = lm_file)
```

Choose predictors only important for the model fitting. Thanks to my classmates' replies, I received some useful suggestion for choosing predictors, which are chi-square, LASSO and lowest RMSE. 
According to my searching result online, chi-square is good for categorical variable as outcome, and LASSO might shrink the coefficients too much. So I will go with the lowest RMSE in the following and see what will happen next.

```{r}
# Load necessary packages
library(caret)

# Split data into training and testing sets
set.seed(123)  # Set seed for reproducibility
trainIndex <- createDataPartition(mydata$BMI, p = 0.8, list = FALSE)
trainData <- mydata[trainIndex, ]
testData <- mydata[-trainIndex, ]

# find important predictors without interactions

# Define predictors
predictors <- c("Gender", "Age", "History", "Water", "Alcohol", "FAF", "MTRANS")

# Initialize variables to store results
minRMSE1 <- Inf
bestSubset1 <- NULL

# Iterate through all possible combinations of predictors
for (i in 1:length(predictors)) {
  subsets1 <- combn(predictors, i)
  for (j in 1:ncol(subsets1)) {
    predictors_subset1 <- subsets1[, j]
    
    # Fit model using subset of predictors
    formula_lm1 <- paste("BMI ~", paste(predictors_subset1, collapse = " + "))
    model_lm1 <- lm(formula_lm1, data = trainData)
    
    # Make predictions on test set
    predictions_lm1 <- predict(model_lm1, newdata = testData)
    
    # Calculate RMSE
    RMSE1 <- sqrt(mean((testData$BMI - predictions_lm1)^2))
    
    # Update minRMSE and bestSubset if RMSE is lower
    if (RMSE1 < minRMSE1) {
      minRMSE1 <- RMSE1
      bestSubset1 <- predictors_subset1
    }
  }
}

# Print results
print(paste("Best subset of predictors:", paste(bestSubset1, collapse = ", ")))
print(paste("Lowest RMSE:", minRMSE1))

# calculate null model

# Compute the mean BMI from the training data
mean_bmi_train <- mean(trainData$BMI)

# Create a vector of predicted values equal to the mean BMI from training data
predicted_bmi_test <- rep(mean_bmi_train, nrow(testData))

# Compute RMSE (Root Mean Squared Error) on test data
rmse_null_test <- sqrt(mean((testData$BMI - predicted_bmi_test)^2))

# Print the results
print(paste("Null Model RMSE on Test Data:", rmse_null_test))
```
```{r}
# Create a data frame for plotting
plot_data1 <- data.frame(Observed = testData$BMI, 
                        Predicted_All = predictions_lm1,
                        Predicted_Null = predicted_bmi_test)

# Plot using ggplot
p1 <- ggplot(plot_data1, aes(x = Observed, y = Predicted_All)) +
  geom_point(color = "blue", size = 2, shape = 16) +
  geom_point(aes(y = Predicted_Null), color = "green", size = 2, shape = 16) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(x = "Observed BMI", y = "Predicted BMI", title = "Observed vs Predicted BMI") +
  theme_minimal() +
  theme(legend.position = "top") +
  scale_shape_manual(values = c(16, 16)) +
  scale_color_manual(values = c("blue", "green")) +
  guides(color = guide_legend(title = "Model"))
plot(p1)

# save the plot
figure_file = here("results","figures","Observed-Predicted1 .png")
ggsave(filename = figure_file, plot=p1) 
```


```{r}
# Define predictors and response variable
predictors <- c("Gender", "Age", "History", "Water", "Alcohol", "FAF", "MTRANS")
interaction_terms <- c("Gender_Age", "FAF_Water")  # Define interaction terms
response <- "BMI"

# Create interaction terms in training and testing data
trainData$Gender_Age <- trainData$Gender * trainData$Age
trainData$FAF_Water <- trainData$FAF * trainData$Water

testData$Gender_Age <- testData$Gender * testData$Age
testData$FAF_Water <- testData$FAF * testData$Water

# Update list of predictors to include interaction terms
predictors_updated <- c(predictors, "Gender_Age", "FAF_Water")

# Initialize variables to store results
minRMSE2 <- Inf
bestSubset2 <- NULL

# Iterate through all possible combinations of predictors
for (i in 1:length(predictors_updated)) {
  subsets2 <- combn(predictors_updated, i)
  for (j in 1:ncol(subsets2)) {
    predictors_subset2 <- subsets2[, j]
    
    # Fit model using subset of predictors
    formula_lm2 <- paste("BMI ~", paste(predictors_subset2, collapse = " + "))
    model_lm2 <- lm(formula_lm2, data = trainData)
    
    # Make predictions on test set
    predictions_lm2 <- predict(model_lm2, newdata = testData)
    
    # Calculate RMSE
    RMSE2 <- sqrt(mean((testData$BMI - predictions_lm2)^2))
    
    # Update minRMSE and bestSubset if RMSE is lower
    if (RMSE2 < minRMSE2) {
      minRMSE2 <- RMSE2
      bestSubset2 <- predictors_subset2
    }
  }
}

# Print results
print(paste("Best subset of predictors (including interactions):", paste(bestSubset2, collapse = ", ")))
print(paste("Lowest RMSE (including interactions):", minRMSE2))
```

```{r}
# Create a data frame for plotting
plot_data2 <- data.frame(Observed = testData$BMI, 
                        Predicted_All = predictions_lm2,
                        Predicted_Null = predicted_bmi_test)

# Plot using ggplot
p2 <- ggplot(plot_data2, aes(x = Observed, y = Predicted_All)) +
  geom_point(color = "red", size = 2, shape = 16) +
  geom_point(aes(y = Predicted_Null), color = "yellow", size = 2, shape = 16) +
  geom_abline(intercept = 0, slope = 1, color = "black") +
  labs(x = "Observed BMI", y = "Predicted BMI", title = "Observed vs Predicted BMI with interactions") +
  theme_minimal() +
  theme(legend.position = "top") +
  scale_shape_manual(values = c(16, 16)) +
  scale_color_manual(values = c("red", "yellow")) +
  guides(color = guide_legend(title = "Model"))
plot(p2)
# save the plot
figure_file = here("results","figures","Observed-Predicted2 .png")
ggsave(filename = figure_file, plot=p2) 
```

It is under my expectation that all-predictor model with the 2 interactions has the lowest RMSE. So I also would like to try LASSO regression model here and see the result.

```{r}
# Load necessary packages
library(glmnet)
library(caret)

# Set seed for reproducibility
set.seed(123)

# Define predictors and response variable
predictors <- c("Gender", "Age", "History", "Water", "Alcohol", "FAF", "MTRANS")
response <- "BMI"

# Prepare data
x <- model.matrix(as.formula(paste(response, "~", paste(predictors, collapse = "+"))), data = mydata)[, -1]  # Exclude intercept column
y <- mydata[, response]

# Define number of folds for cross-validation
nfolds <- 10

# Perform cross-validation
cv_model <- cv.glmnet(x, y, alpha = 1, nfolds = nfolds)  # Use alpha = 1 for LASSO regression

# Extract selected predictors
selected_predictors <- rownames(coef(cv_model, s = "lambda.min"))[-1]  # Exclude intercept column

# Print selected predictors
print(paste("Selected predictors:", paste(selected_predictors, collapse = ", ")))
```

The LASSO regression result includes some specific levels in variables instead of the entire variables. 

Using CV to test evaluate the model with all predictors.

```{r}
# Define predictors and response variable
predictors <- c("Gender", "Age", "History", "Water", "Alcohol", "FAF", "MTRANS")
interaction_terms <- c("Gender_Age", "FAF_Water")
response <- "BMI"

# Create interaction terms in data
mydata$Gender_Age <- mydata$Gender * mydata$Age
mydata$FAF_Water <- mydata$FAF * mydata$Water

# Define training control with repeated k-fold cross-validation
train_control1 <- trainControl(method = "repeatedcv", 
                              number = 10,       # Number of folds
                              repeats = 3)       # Number of repetitions

# Define the model formula
formula_CV <- as.formula(paste(response, "~", paste(c(predictors, interaction_terms), collapse = " + ")))

# Train the model using cross-validation
model <- train(formula_CV,
               data = mydata,
               method = "lm",                    # Use linear regression
               trControl = train_control1,       # Use defined training control
               preProcess = c("center", "scale"),  # Preprocess data
               metric = "RMSE")                    # Use RMSE as the metric

# Print the cross-validation results
print(model)
```

I will also try to use obesity level which is categorical variable so I can use DA models to fit the data and might compare the result with linear models. There are totally 7 categories classified by the original authors of the dataset.

```{r}
library(MASS)
library(caret)

# Define the outcome variable
outcome <- "Obesity"

# Define predictor variables
predictors <- c("Gender", "Age", "History", "Water", "Alcohol", "FAF", "MTRANS")

# Create a training/testing split
set.seed(123)  # Set seed for reproducibility
trainIndex3 <- createDataPartition(mydata$Obesity, p = 0.8, list = FALSE)
trainData3 <- mydata[trainIndex, ]
testData3 <- mydata[-trainIndex, ]

# Initialize variables to store results
best_subset_DA <- NULL
best_accuracy_DA <- 0

# Iterate through all possible combinations of predictors
for (i in 1:length(predictors)) {
  subsets3 <- combn(predictors, i)
  for (j in 1:ncol(subsets3)) {
    predictors_subset_DA <- subsets3[, j]
    
    # Fit the DA model using the subset of predictors
    formula_DA <- as.formula(paste(outcome, "~", paste(predictors_subset_DA, collapse = " + ")))
    model_DA <- lda(formula_DA, data = trainData3)
    
    # Make predictions on the testing set
    predicted_DA <- predict(model_DA, newdata = testData3)$class
    
    # Calculate accuracy
    accuracy_DA <- mean(predicted_DA == testData3$Obesity)
    
    # Update best subset and accuracy if current accuracy is higher
    if (accuracy_DA > best_accuracy_DA) {
      best_subset_DA <- predictors_subset_DA
      best_accuracy_DA <- accuracy_DA
    }
  }
}

# Print the best subset of predictors and accuracy
print(paste("Best subset of predictors:", paste(best_subset_DA, collapse = ", ")))
print(paste("Best accuracy:", best_accuracy_DA))
```

Adding 2 interactions to the DA model.

```{r}
# Define the outcome variable
outcome <- "Obesity"

# Update list of predictors to include interaction terms
predictors_updated <- c(predictors, "Gender_Age", "FAF_Water")

# Create a training/testing split
set.seed(123)  # Set seed for reproducibility
trainIndex4 <- createDataPartition(mydata$Obesity, p = 0.8, list = FALSE)
trainData4 <- mydata[trainIndex, ]
testData4 <- mydata[-trainIndex, ]

# Create interaction terms in training and testing data
trainData4$Gender_Age <- trainData4$Gender * trainData4$Age
trainData4$FAF_Water <- trainData4$FAF * trainData4$Water

testData4$Gender_Age <- testData4$Gender * testData4$Age
testData4$FAF_Water <- testData4$FAF * testData4$Water

# Initialize variables to store results
best_subset_DA_U <- NULL
best_accuracy_DA_U <- 0

# Iterate through all possible combinations of predictors
for (i in 1:length(predictors_updated)) {
  subsets4 <- combn(predictors_updated, i)
  for (j in 1:ncol(subsets4)) {
    predictors_subset_DA_U <- subsets4[, j]
    
    # Fit the DA model using the subset of predictors
    formula_DA_U <- as.formula(paste(outcome, "~", paste(predictors_subset_DA_U, collapse = " + ")))
    model_DA_U <- lda(formula_DA_U, data = trainData4)
    
    # Make predictions on the testing set
    predicted_DA_U <- predict(model_DA_U, newdata = testData4)$class
    
    # Calculate accuracy
    accuracy_DA_U <- mean(predicted_DA_U == testData4$Obesity)
    
    # Update best subset and accuracy if current accuracy is higher
    if (accuracy_DA_U > best_accuracy_DA_U) {
      best_subset_DA_U <- predictors_subset_DA_U
      best_accuracy_DA_U <- accuracy_DA_U
    }
  }
}

# Print the best subset of predictors and accuracy
print(paste("Best subset of predictors:", paste(best_subset_DA_U, collapse = ", ")))
print(paste("Best accuracy:", best_accuracy_DA_U))
```
After adding 2 interactions, the accuracy increased a little bit.

Using cross validation to evaluate the model.

```{r}
# Define predictors and response variable
mydata$Obesity <- factor(mydata$Obesity)
predictors <- c("Gender", "Age", "History", "Water", "Alcohol", "FAF", "MTRANS")
interaction_terms <- c("Gender_Age", "FAF_Water")
outcome <- "Obesity"

# Define the model formula
formula_CV_DA <- as.formula(paste(outcome, "~", paste(c(predictors, interaction_terms), collapse = " + ")))

# Set seed for reproducibility
set.seed(1234)

# Create 10-fold cross-validation folds
new_folds <- vfold_cv(mydata, v = 10)

# Define model specification
lda_spec <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")

# Workflow for model with specified predictors
lda_model <- workflow() %>%
  add_model(lda_spec) %>%
  add_formula(formula_CV_DA)

# Workflow for model using all predictors
lda_wf <- workflow() %>%
  add_model(lda_spec) %>%
  add_formula(formula_CV_DA)

# Perform CV model training and evaluation
lda_CV <- lda_wf %>%
  fit_resamples(resamples = new_folds, metrics = metric_set(accuracy))

# Collect RMSE values
collect_metrics(lda_CV)
```

# Notes

For your own explorations, tables and figures can be "quick and dirty". As long as you can see what's going on, there is no need to polish them. That's in contrast to figures you'll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible.


