---
title: "Project eda"
author: "Xueyan Hu"
date: "2023-02-22"
output: html_document
---

  
  
This Quarto file loads the cleaned data and does some exploring.

I'm only showing it the way where the code is included in the file. 
As described in the `processing_code` materials, I currently prefer the approach of having R code in a separate file and pulling it in.

But I already had this written and haven't yet re-done it that way. Feel free to redo and send a pull request on GitHub :)

Again, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files.
And sometimes, an R script with enough comments is good enough and one doesn't need a Quarto file.

Also note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you'll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.

As part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.

Start by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables. 

Plots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.

# Setup

```{r}
#load needed packages. make sure they are installed.
library(here) #for data loading/saving
library(dplyr)
library(skimr)
library(ggplot2)
library(maps)
library(gt)
```


Load the data.

```{r}
#Path to data. Note the use of the here() package and not absolute paths
data_location <- here::here("data","processed-data","processeddata_cancer.rds")
#load data
mydata <- readRDS(data_location)
```

# bar chart for crude prevalence according to states

```{r}
# Group by state and calculate the total population and cancer cases
state_total <- mydata %>%
  group_by(StateDesc) %>%
  summarize(TotalPopulation = sum(PopulationCount),
            TotalCancerCases = sum(Cancer_Case))

# Calculate the prevalence for each state
state_total <- state_total %>%
  mutate(TotalPrevalence = TotalCancerCases / TotalPopulation)

# View the resulting dataframe
print(state_total)

# Load usmap package and get the state data
library(usmap)

usmap_state_data <- us_map(regions = "states")

# There are a few state names that are cut off in state_total, but the column
# is already alphabetical so we can replace it.
# To use plot_usmap() we have to have a column named `state` or `fips`.
state_total$state <- usmap_state_data$full
print(state_total)

# Now we can try to make a usmap plot
plot_usmap("states", data = state_total, values = "TotalPrevalence") +
  scale_fill_viridis_c()
```

# List 10 cities that has the highest cancer prevalence

```{r}
top_10_highest <- mydata %>%
  top_n(10, Data_Value) %>%
  select(Year, StateAbbr, CityName, Data_Value)

print(top_10_highest)

# create a table
table <- top_10_highest %>%
  gt() %>%
  tab_spanner(
    label = md('**Year**'),
    columns = c('Year')
  ) %>%
  tab_spanner(
    label = md('**State**'),
    columns = c('StateAbbr')
  ) %>%
  tab_spanner(
    label =  md('**Cancer Prevalence**'),
    columns = c('Data_Value')
  ) %>%
  tab_header(
    title = '2017 top 10 cities which have highest cancer rates'
  ) %>%
  opt_stylize(style = 6, color = 'gray')
```

We are saving the results to the `results` folder. Depending on how many tables/figures you have, it might make sense to have separate folders for each. And/or you could have separate folders for exploratory tables/figures and for final tables/figures. Just choose a setup that makes sense for your project and works for you, and provide enough documentation that someone can understand what you are doing.

# fit a simple linear model

Before we started analysis, we had an assumption 

```{r}
# Fit linear regression model
lm1 <- lm(Data_Value ~ PopulationCount, data = mydata)

# Print summary of the model
summary(lm1)
```
So the hypothesis is not supported by the linear model fitting result. There is no obvious evidence that the cancer prevalence in larger cities (higher population) 



# Notes

For your own explorations, tables and figures can be "quick and dirty". As long as you can see what's going on, there is no need to polish them. That's in contrast to figures you'll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible.


